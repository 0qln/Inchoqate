https://stackoverflow.com/questions/31081555/opengl-gpu-memory-exceeded-possible-scenarios

Scenario:
4k Image (3840 x 2160 = 8294400 pixels = 33177600 B ~= 32 MB)
=> 32MB per node
rtx2060 (6 GB GPU ram = 6144 MB available; 30% used = 4300 MB available)
=> 4300MB / 32BM = 134 Graph Nodes
    before the before OpenGL has to lay of texture buffers to vram...
    at which point the graphics will likely begin to stutter.
=> It's realistic to give a buffer to each node.


Reusing nodes:
Computing the graphs result in an iterative, breath-first fashion, rather
recursivly, depth-first allows for reusable buffers.

=> If we reuse buffers we will practically never run out of GPU memory .
